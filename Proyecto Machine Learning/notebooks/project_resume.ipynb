{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESUMEN PROYECTO MACHINE LEARNING\n",
    "## ESPERANZA DE VIDA: PREDICIÓN SIN EDADES\n",
    "### ADRIAN GARCIA HERNANDEZ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de los datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos han sido obtenidos de la página web del Banco Mundial, más específicamente se trata de los **Indicadores de Desarrollo Mundial**. Presenta los datos de desarrollo global más actuales y precisos disponibles, e incluye estimaciones nacionales, regionales y globales. Los indicadores para cada país son muy numerosos (489 indicadores por país) y con una periodicidad anual que va desde 1960 a 2020. \n",
    "\n",
    "La existencia de una gran cantidad de valores nulos ha sido la principal razón por la que se ha cambiado el tipo de análisis inicialmente contemplado. Se han escogido los valores de los años 2000 a 2019, ya que son aquellos que menos valores nulos presentan de toda la serie. Se han anonimizado los valores (eliminado los nombres de los países) y se han añadido por cada país y año una nueva fila que contiene los valores. Por lo tanto, hemos pasado de tener un *panel data* a un *cross-sectional data*. Esto podría suponer un problema ya que realmente los datos que tenemos no se basan en un año en concreto, si no que en realidad están repartidos en un periodo de 20 años. Este problema se puede en parte corregir al hacer uso de variables que no incluyen perturbaciones que no sean incluidos en el dataset, como puede ser la inflación y su efecto en los precios; por ello se hace uso del PIB a precios constantes (US$ a precios de 2015)\n",
    "\n",
    "\n",
    "El dataset completo puede ser consultado [aqui](https://datacatalog.worldbank.org/search/dataset/0037712/World-Development-Indicators)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos & Merging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset estaba estructurado de la siguiente manera, por una parte las columnas describian los años, desde 1960 hasta 2020, por otra las filas en las filas se incluia cada variable posible por cada pais del mundo, resultando en algo más de 380.000 filas. Debido a la gran falta de datos, elegimos únicamente el periodo desde 2000 al 2019, los años con más datos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la conversión de un panel data a un -falso- cross-sectional data:\n",
    "\n",
    "- Primeramente seleccionamos la primera columna del año escogido asi como las que identifican el país y se le cambia el label del año por 'Value':\n",
    "\n",
    "    ```python\n",
    "    columns_to_keep = ['Country Code', 'Indicator Code', '2001']\n",
    "    df = df.loc[:, columns_to_keep]\n",
    "    df = df.rename(columns={'2001': 'Value'})\n",
    "    ```\n",
    "\n",
    "\n",
    "- Posteriormente pivotamos el dataframe de la siguiente manera:\n",
    "    ```python\n",
    "    df = df.pivot(index='Country Code', columns='Indicator Code', values='Value')\n",
    "    ```\n",
    "\n",
    "\n",
    "- Reseteamos el índice:\n",
    "    ```python\n",
    "    df.reset_index(inplace=True)\n",
    "    ```\n",
    "\n",
    "- Guardamos el dataframe resultante en un nuevo csv con el nombre de data y el año que representa los valores:\n",
    "    ```python\n",
    "    df.to_csv('data_2001.csv', index=False)\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez contamos con todos los CSV de cada año desde el 2000 al 2019, debemos unirlos en un único dataframe.\n",
    "\n",
    "Para ello seguimos los siguientes pasos:\n",
    "\n",
    "- Creamos el path a la carpeta donde se localizan todos los CSV y creamos una lista vacia llamada data_frames:\n",
    "    ```python\n",
    "    path = \"Data merging/\"\n",
    "    data_frames = []\n",
    "    ````\n",
    "- Creamos un bucle for para que recoja todos los archivos CSV dentro de la carpeta y agreagrlos dentro de la lista data_frames:\n",
    "\n",
    "    ```python\n",
    "    import os\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(os.path.join(path, file))\n",
    "            data_frames.append(df)\n",
    "\n",
    "    ```\n",
    "- Finalmente usando la funcion concat, unimos todo en un mismo dataframe:\n",
    "    ```python\n",
    "    df = pd.concat(data_frames, ignore_index=True)\n",
    "    ```\n",
    "\n",
    "- Hacemos limpieza de aquellas columnas y filas que tengan demasiados NaNs:\n",
    "    ```python\n",
    "    df = df.dropna(thresh=len(df) - 200, axis=1)\n",
    "\n",
    "    df = df.dropna(thresh=len(df.columns) - 5, axis=0)\n",
    "    ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis exploratorio & Feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pese a deber ser el primer paso a hacer, hasta que no se han hecho todas las transformaciones anteriores no se podia realmente visualizar de manera clara el dataset. Tras estas, se han calculado los principales parámetros, observado la matriz de correlación y ploteado diferentes gráficos para una buena comprensión de los datos que presenta el dataset. \n",
    "\n",
    "Al principio se abordó la posibilidad de sustituir los NaNs por la mediana de la columna. Tras comparar los diferentes modelos y comprobar su peor rendimiento se decidió eliminar todos los NaNs. \n",
    "\n",
    "Respecto al escalado de los datos, se probaron diferentes formas como por ejemplo, el StandardScaler o el MinMaxScaler, no obstante estos tambien afectaban negatívamente al rendimiento de los modelos. Además, afectaban a la correcta comprensión de del MSE o el R2, haciendo necesario calcular estos su magnitud natural."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se han probado diversos modelos y calculado sus respectivas métricas para comprobar su rendimiento. \n",
    "Se han considerado los siguientes modelos: Linear Regression, Decision Tree, Random Forest, Adaboost, Gradient Boosting, XGBoost (con y sin convertir la variable dependiente a logarítmica). También se han probado modelos de regresión lineal regularizados usando los métodos: ElasticNet, Ridge, Lasso y Bayesian Ridge; y se han buscado los mejores hiperparámetros de estos. Por otra parte, también se ha hecho uso de un pipeline para la normalización de los datos con StandardScaler y la búsqueda de hiperparámetros del modelo KernelRidge usando GridSearchCV.\n",
    "\n",
    "Finalmente, tras comparar los resultados de cada uno de los modelos se ha determinado que aquel con mayor precisión es el **Random Forest**.\n",
    "\n",
    "Este ha obtenido un error cuadrático medio igual a 1.53 lo que indica que en promedio, las predicciones del modelo difieren en 1.53 años del valor real de X. Un MSE de 1.53 años es considerado relativamente bajo, lo que sugiere que el modelo puede estar haciendo buenas predicciones en general.\n",
    "\n",
    "Ha obtenido un coeficiente de determinación(R2) igual a 0.98, esto indica que aproximadamente el 98% de la variabilidad de la variable dependiente(la que predecimos) se puede explicar por la relación con las variables independientes incluidas en el modelo, lo que sugiere que el modelo tiene un buen ajuste a los datos.\n",
    "\n",
    "Para confirmar la fiabilidad del modelo también se deben de comprobar que los residuos no tengan autocorrelación, heterocedasticidad, no estacionariedad  y que estos sigan una distribución normal.\n",
    "\n",
    "Para comprobar la autocorrelación se ha llevado a cabo el test de **Durbin Watson**, el cual ha demostrado que no hay una autocorrelación significativa en los residuos. Para comprobar que no haya heterocedasticidad, se ha hecho el test de **Breush-Pagan**, confirmando la homocedasticidad de los residuos. Respecto a la no estacionariedad(no Unit Root) se ha probado el test de **Dickey-Fuller** el cual ha rechazado la hipótesis nula, es decir, que los residuos no tienen Unit Root, o lo que es lo mismo, que los residuos son estacionarios. Por último, se ha comprobado la normalidad en la distribución de los residuos; para ello se han hecho dos tests, el de **Shapiro-Wilk** y el de **Kolmogorov-Smirnov**. Ambos tests han determinado que los residuos **NO** se distribuyen de manera normal. Esto puede suponer un problema ya que se podría estar violando alguna suposición del modelo. No obstante, al tratarse de una muestra muy grande (n>100) podemos hacer uso del **Teorema Central del Límite** y asumir que se distribuyen de una manera aproximadamente Normal.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui debajo se especifica el código del modelo finalmente escogido:\n",
    "\n",
    "\n",
    "```python\n",
    "models = {\"Random Forest\": RandomForestRegressor()}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Print the model's performance metrics\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean Squared Error: {mse:.5f}\")\n",
    "    print(f\"  Root Mean Squared Error: {rmse:.2f}\")\n",
    "    print(f\"  R-squared: {r2:.2f}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código de los tests realizados:\n",
    "\n",
    "\n",
    "- **TEST AUTOCORRELACIÓN DURBING-WATSON**\n",
    "\n",
    "    ```python\n",
    "    from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the residuals\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    # Calculate the Durbin-Watson statistic\n",
    "    durbin_watson_statistic = durbin_watson(residuals)\n",
    "\n",
    "    # Print the Durbin-Watson statistic\n",
    "    print(\"Durbin-Watson statistic:\")\n",
    "    print(f\"  {durbin_watson_statistic:.3f}\")\n",
    "\n",
    "    # Interpret the Durbin-Watson statistic\n",
    "    if durbin_watson_statistic < 1.5:\n",
    "        print(\"There may be positive autocorrelation in the residuals.\")\n",
    "    elif durbin_watson_statistic > 2.5:\n",
    "        print(\"There may be negative autocorrelation in the residuals.\")\n",
    "    else:\n",
    "        print(\"There is no significant autocorrelation in the residuals.\")\n",
    "    ```\n",
    "- **TEST HETEROCEDASTICIDAD BREUSCH-PAGAN**\n",
    "\n",
    "    ```python\n",
    "    from statsmodels.api import add_constant\n",
    "    from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the residuals\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    # Add a constant column to X_test\n",
    "    X_test_with_const = add_constant(X_test)\n",
    "\n",
    "    # Calculate the test statistic and p-value for the Breusch-Pagan test\n",
    "    test_stat, p_value, _, _ = het_breuschpagan(residuals, X_test_with_const)\n",
    "\n",
    "    # Print the test statistic and p-value\n",
    "    print(\"Breusch-Pagan test for homoscedasticity:\")\n",
    "    print(f\"  Test Statistic: {test_stat:.3f}\")\n",
    "    print(f\"  p-value: {p_value:.3f}\")\n",
    "\n",
    "    # Interpret the test results\n",
    "    if p_value < 0.05:\n",
    "        print(\"The residuals are not homoscedastic.\")\n",
    "    else:\n",
    "        print(\"The residuals are homoscedastic.\")\n",
    "    ```\n",
    "- **TEST NO ESTACIONARIEDAD DICKEY-FULLER**\n",
    "    ```python\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the residuals\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    # Apply the Dickey-Fuller test to the residuals\n",
    "    adf_test = adfuller(residuals)\n",
    "\n",
    "    # Print the test statistic and p-value\n",
    "    print(\"Dickey-Fuller test:\")\n",
    "    print(f\"  Test statistic: {adf_test[0]:.3f}\")\n",
    "    print(f\"  p-value: {adf_test[1]:.3f}\")\n",
    "\n",
    "    # Interpret the test result\n",
    "    if adf_test[1] < 0.05:\n",
    "        print(\"Reject the null hypothesis. The residuals do not have a unit root.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis. The residuals may have a unit root.\")\n",
    "    ```\n",
    "- **TEST NORMALIDAD DE LOS RESIDUOS**\n",
    "\n",
    "    - **Kolmogorov-Smirnov**\n",
    "        ```python\n",
    "        from scipy.stats import kstest\n",
    "\n",
    "        # Calculate the residuals\n",
    "        residuals = y_test - y_pred\n",
    "\n",
    "        # Perform the K-S test for normality\n",
    "        stat, p = kstest(residuals, \"norm\")\n",
    "\n",
    "        # Print the test statistic and p-value\n",
    "        print(\"Kolmogorov-Smirnov test for normality:\")\n",
    "        print(f\"  Test Statistic: {stat:.3f}\")\n",
    "        print(f\"  p-value: {p:.3f}\")\n",
    "\n",
    "        # Interpret the test results\n",
    "        if p < 0.05:\n",
    "            print(\"The residuals are not normally distributed.\")\n",
    "        else:\n",
    "            print(\"The residuals are normally distributed.\")\n",
    "        ```\n",
    "    - **Shapiro-Wilk**\n",
    "        ```python\n",
    "        from scipy.stats import shapiro\n",
    "\n",
    "        # Calculate the residuals\n",
    "        residuals = y_test - y_pred\n",
    "\n",
    "        # Perform the Shapiro-Wilk test for normality\n",
    "        stat, p = shapiro(residuals)\n",
    "\n",
    "        # Print the test statistic and p-value\n",
    "        print(\"Shapiro-Wilk test for normality:\")\n",
    "        print(f\"  Test Statistic: {stat:.3f}\")\n",
    "        print(f\"  p-value: {p:.5f}\")\n",
    "\n",
    "        # Interpret the test results\n",
    "        if p < 0.005:\n",
    "            print(\"The residuals are not normally distributed.\")\n",
    "        else:\n",
    "            print(\"The residuals are normally distributed.\")\n",
    "        ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras la elección del modelo (Random forest), se ha procedido a la creación de dor archivos python, preprocessing.py y train.py.\n",
    "\n",
    "- **preprocessing.py:** Crea una función llamada *load_data* que importa el dataset en un pandas dataframe y selecciona las columnas necesarias para el modelo.\n",
    "\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    def load_data():\n",
    "        df = pd.read_csv('/Users/adriangarcia/Desktop/Project_ML_Adri/data/processed/merged_20_years.csv')  \n",
    "        df = df[['SP.DYN.LE00.IN','NY.GDP.MKTP.KD','SP.POP.TOTL','AG.SRF.TOTL.K2']]\n",
    "        df = df.dropna()\n",
    "        return df\n",
    "\n",
    "    ```\n",
    "- **train.py:** Llama al archivo *preprocessing.py* para cargar el dataset usando la función *load_data* previamente creada y entrena el modelo, finalmente guardando el modelo en un archivo *new_model.joblib*.\n",
    "\n",
    "    ```python\n",
    "    import preprocessing\n",
    "    import joblib\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = preprocessing.load_data()\n",
    "\n",
    "    X = df[['NY.GDP.MKTP.KD','SP.POP.TOTL','AG.SRF.TOTL.K2']]\n",
    "    y = df['SP.DYN.LE00.IN']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 42)\n",
    "\n",
    "    models = {\"Random Forest\": RandomForestRegressor()}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    # Train and save the model\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    joblib.dump(model, 'new_model.joblib')\n",
    "\n",
    "    ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
